{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a9aed5",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will verify the custom torch extension kernel with\n",
    "- The correctness.\n",
    "- The peak usage memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f0b958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import torch\n",
    "import torch_cuda_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1591aea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PyTorch Info ===\n",
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "torch.version.cuda: 12.1\n",
      "GPU name: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== PyTorch Info ===\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"torch.version.cuda: {torch.version.cuda}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf139710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_memory_profiler(device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Decorator that measures GPU memory usage (and runtime) for any function.\n",
    "    \n",
    "    Reports:\n",
    "      - Δpeak (max temporary memory used)\n",
    "      - Δcurrent (net memory retained after execution)\n",
    "      - runtime (optional)\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # synchronize before measuring\n",
    "            torch.cuda.synchronize(device)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats(device)\n",
    "            \n",
    "            before = torch.cuda.memory_allocated(device)\n",
    "\n",
    "            result = func(*args, **kwargs)  # run the function\n",
    "\n",
    "            torch.cuda.synchronize(device)\n",
    "            peak = torch.cuda.max_memory_allocated(device)\n",
    "            \n",
    "            delta_peak = peak - before\n",
    "\n",
    "            msg = (f\"[{func.__name__}] Δpeak: {delta_peak/1e6:.2f} MB\")\n",
    "            print(msg)\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def cuda_time_profiler(print_time=True):\n",
    "    \"\"\"\n",
    "    Decorator to measure GPU execution time of a function using CUDA events.\n",
    "    Works only if at least one tensor is on the CUDA device.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            \n",
    "            # Warm-up\n",
    "            func(*args, **kwargs)\n",
    "            torch.cuda.synchronize() # Ensure all previous CUDA ops are done\n",
    "\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            n_iter = 10\n",
    "\n",
    "            start.record()\n",
    "            for i in range(n_iter):\n",
    "                result = func(*args, **kwargs)\n",
    "            end.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            elapsed_ms = start.elapsed_time(end) / n_iter  # Average time per iteration\n",
    "            if print_time:\n",
    "                print(f\"[{func.__name__}] elapsed: {elapsed_ms:.3f} ms\")\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def l2_norm(tensor1, tensor2):\n",
    "    return torch.sqrt(torch.nansum((tensor1 - tensor2) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166e997",
   "metadata": {},
   "source": [
    "# 1. Check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9264d7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correct float matmul implementation! ---\n"
     ]
    }
   ],
   "source": [
    "# The float matmul test\n",
    "A = torch.randn(1_000, 2_000, device=\"cuda\")\n",
    "B = torch.randn(2_000, 3_000, device=\"cuda\")\n",
    "\n",
    "custom_matmul = torch_cuda_ext.matmul_f32(A, B)\n",
    "torch_matmul = torch.matmul(A, B)\n",
    "\n",
    "if torch.allclose(custom_matmul, torch_matmul, atol=1e-3):\n",
    "    print(\"--- Correct float matmul implementation! ---\")\n",
    "else:\n",
    "    print(\"[ERROR] Incorrect float matmul implementation !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622a9609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correct CUTLASS implementation! ---\n"
     ]
    }
   ],
   "source": [
    "# The float matmul CUTLASS test\n",
    "A = torch.randn(1_000, 2_000, device=\"cuda\")\n",
    "B = torch.randn(2_000, 3_000, device=\"cuda\")\n",
    "\n",
    "custom_matmul_cutlass = torch_cuda_ext.matmul_f32_cutlass(A, B)\n",
    "torch_matmul = torch.matmul(A, B)\n",
    "\n",
    "if torch.allclose(custom_matmul_cutlass, torch_matmul, atol=1e-3):\n",
    "    print(\"--- Correct CUTLASS implementation! ---\")\n",
    "else:\n",
    "    print(\"[ERROR] Incorrect float matmul CUTLASS implementation !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a669c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correct implementation! ---\n"
     ]
    }
   ],
   "source": [
    "# The int8 matmul test\n",
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "custom_matmul_int8 = torch_cuda_ext.matmul_int8(A, B)\n",
    "torch_matmul_int8 = torch.matmul(A.float(), B.float())\n",
    "\n",
    "if torch.allclose(custom_matmul_int8.float(), torch_matmul_int8, atol=1e-2):\n",
    "    print(\"--- Correct implementation! ---\")\n",
    "else:\n",
    "    print(\"[ERROR] Incorrect int8 matmul implementation !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bed6b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correct CUTLASS implementation! ---\n"
     ]
    }
   ],
   "source": [
    "# The int8 matmul CUTLASS test\n",
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "custom_matmul_int8_cutlass = torch_cuda_ext.matmul_int8_cutlass(A, B)\n",
    "torch_matmul_int8 = torch.matmul(A.float(), B.float())\n",
    "\n",
    "if torch.allclose(custom_matmul_int8_cutlass.float(), torch_matmul_int8, atol=1e-2):\n",
    "    print(\"--- Correct CUTLASS implementation! ---\")\n",
    "else:\n",
    "    print(\"[ERROR] Incorrect int8 matmul CUTLASS implementation !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb2a5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n",
      "torch.Size([1000, 3000])\n"
     ]
    }
   ],
   "source": [
    "# The int8 matmul + convert to float test\n",
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "custom_matmul_int8_f16 = torch_cuda_ext.matmul_int8_to_fp16_scaled_forward_noc(A, B, 1.0)\n",
    "print(custom_matmul_int8_f16.dtype)\n",
    "print(custom_matmul_int8_f16.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a76fb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correct implementation! ---\n"
     ]
    }
   ],
   "source": [
    "# The batched int8 matmul test\n",
    "A = torch.randint(-127, 127, (10, 1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (10, 2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "# custom_bmatmul_int8 = torch_cuda_ext.bmm_int8(A, B)\n",
    "custom_bmatmul_int8 = torch_cuda_ext.bmm_int8_cutlass_forward_streams(A, B)\n",
    "torch_bmatmul_int8 = torch.bmm(A.float(), B.float())        \n",
    "\n",
    "if torch.allclose(custom_bmatmul_int8.float(), torch_bmatmul_int8, atol=1e-2):\n",
    "    print(\"--- Correct implementation! ---\")\n",
    "else:\n",
    "    print(\"[ERROR] Incorrect int8 batched matmul implementation !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238c28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9486f2f",
   "metadata": {},
   "source": [
    "# 2. Check memory peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fed27e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_matmul_f32] Δpeak: 12.45 MB\n",
      "\n",
      "[torch_built_int_matmul_f32] Δpeak: 12.00 MB\n"
     ]
    }
   ],
   "source": [
    "# Compare float matmul and torch matmul\n",
    "A = torch.randn(1_000, 2_000, device=\"cuda\")\n",
    "B = torch.randn(2_000, 3_000, device=\"cuda\")\n",
    "\n",
    "@cuda_memory_profiler(device=\"cuda\")\n",
    "def custom_matmul_f32(A, B):\n",
    "    return torch_cuda_ext.matmul_f32_cutlass(A, B)\n",
    "\n",
    "_ = custom_matmul_f32(A, B)\n",
    "print()\n",
    "\n",
    "@cuda_memory_profiler(device=\"cuda\")\n",
    "def torch_built_int_matmul_f32(A, B):\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "_ = torch_built_int_matmul_f32(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f60eab5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_matmul_int8_cutlass] Δpeak: 12.00 MB\n",
      "\n",
      "[torch_batched_matmul_int8] Δpeak: 6.45 MB\n"
     ]
    }
   ],
   "source": [
    "# Compare matmul memory peak\n",
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "@cuda_memory_profiler()\n",
    "def custom_matmul_int8_cutlass(A, B):\n",
    "    return torch_cuda_ext.matmul_int8_cutlass(A, B)\n",
    "\n",
    "_ = custom_matmul_int8_cutlass(A, B)\n",
    "print()\n",
    "\n",
    "# =============== Torch batched matmul =================\n",
    "A = A.to(torch.float16)\n",
    "B = B.to(torch.float16)\n",
    "@cuda_memory_profiler()\n",
    "def torch_batched_matmul_int8(A, B):\n",
    "    return torch.matmul(A, B)\n",
    "\n",
    "_ = torch_batched_matmul_int8(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb6340ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_batched_matmul_int8_cutlass] Δpeak: 132.00 MB\n",
      "\n",
      "[torch_batched_matmul_int8] Δpeak: 60.82 MB\n"
     ]
    }
   ],
   "source": [
    "# Compare matmul memory peak\n",
    "A = torch.randint(-127, 127, (10, 1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (10, 2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "@cuda_memory_profiler()\n",
    "def custom_batched_matmul_int8_cutlass(A, B):\n",
    "    return torch_cuda_ext.bmm_int8_cutlass_forward_streams(A, B)\n",
    "\n",
    "_ = custom_batched_matmul_int8_cutlass(A, B)\n",
    "print()\n",
    "\n",
    "# =============== Torch batched matmul =================\n",
    "A = A.to(torch.float16)\n",
    "B = B.to(torch.float16)\n",
    "\n",
    "@cuda_memory_profiler()\n",
    "def torch_batched_matmul_int8(A, B):\n",
    "    return torch.bmm(A, B)\n",
    "\n",
    "_ = torch_batched_matmul_int8(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2612a2ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d1b9a59",
   "metadata": {},
   "source": [
    "# 3. Check time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85717cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_bmm_int8] elapsed: 0.264 ms \n",
      "\n",
      "[torch matmul] elapsed: 0.197 ms\n"
     ]
    }
   ],
   "source": [
    "# Compare time of int8 matmul\n",
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = torch_cuda_ext.matmul_int8_cutlass(A, B)\n",
    "    _ = torch.matmul(A.float(), B.float())\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "n_iter = 100\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "for _ in range(n_iter):\n",
    "    _ = torch_cuda_ext.matmul_int8_cutlass(A, B)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "custom_time = start.elapsed_time(end) / n_iter  # in ms\n",
    "print(f\"[custom_bmm_int8] elapsed: {custom_time:.3f} ms \\n\")\n",
    "\n",
    "\n",
    "# =============== Torch matmul =================\n",
    "A = A.to(torch.float16)\n",
    "B = B.to(torch.float16)\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "for _ in range(n_iter):\n",
    "    _ = torch.matmul(A, B)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "torch_time = start.elapsed_time(end) / n_iter  # in ms\n",
    "print(f\"[torch matmul] elapsed: {torch_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d1a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_bmm_int8] elapsed: 0.685 ms \n",
      "\n",
      "[torch batched matmul] elapsed: 0.256 ms\n"
     ]
    }
   ],
   "source": [
    "# Compare time of batched int8 matmul\n",
    "A = torch.randint(-127, 127, (16, 1024, 512), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (16, 512, 1024), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = torch_cuda_ext.bmm_int8_cutlass_forward_streams(A, B)\n",
    "    _ = torch.bmm(A.float(), B.float())\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "n_iter = 100\n",
    "start = time.time()\n",
    "for _ in range(n_iter):\n",
    "    _ = torch_cuda_ext.bmm_int8_cutlass_forward_streams(A, B)\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "custom_time = (end - start) / n_iter * 1e3  # in ms\n",
    "print(f\"[custom_bmm_int8] elapsed: {custom_time:.3f} ms \\n\")\n",
    "\n",
    "A = A.to(torch.float16)\n",
    "B = B.to(torch.float16)\n",
    "start = time.time()\n",
    "for _ in range(n_iter):\n",
    "    _ = torch.bmm(A, B)\n",
    "    torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "torch_time = (end - start) / n_iter * 1e3  # in ms\n",
    "print(f\"[torch batched matmul] elapsed: {torch_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037acb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "242580d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_bmm_int8] elapsed: 0.669 ms \n",
      "\n",
      "[torch batched matmul] elapsed: 0.251 ms\n"
     ]
    }
   ],
   "source": [
    "# Compare time of batched int8 matmul\n",
    "A = torch.randint(-127, 127, (16, 1024, 512), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (16, 512, 1024), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = torch_cuda_ext.bmm_int8_cutlass_forward_streams(A, B)\n",
    "    _ = torch.bmm(A.float(), B.float())\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "n_iter = 100\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "for _ in range(n_iter):\n",
    "    _ = torch_cuda_ext.bmm_int8_cutlass_forward_streams(A, B)\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "custom_time = start.elapsed_time(end) / n_iter  # in ms\n",
    "print(f\"[custom_bmm_int8] elapsed: {custom_time:.3f} ms \\n\")\n",
    "\n",
    "# ================ Torch batched matmul =================\n",
    "A_float = A.to(torch.float16)\n",
    "B_float = B.to(torch.float16)\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "for _ in range(n_iter):\n",
    "    _ = torch.bmm(A_float, B_float)\n",
    "    \n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "torch_time = start.elapsed_time(end) / n_iter  # in ms\n",
    "print(f\"[torch batched matmul] elapsed: {torch_time:.3f} ms\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134666ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f439ad7",
   "metadata": {},
   "source": [
    "## 3.1. Check the fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b747a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_matmul_int8_to_fp16] elapsed: 0.275 ms \n",
      "\n",
      "torch.float16\n",
      "torch.Size([1000, 3000])\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "# Measure time of int8 matmul + convert to float16\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = torch_cuda_ext.matmul_int8_to_fp16_scaled_forward_noc(A, B, 1.0)\n",
    "torch.cuda.synchronize()\n",
    "    \n",
    "n_iter = 100\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iter):    \n",
    "    out = torch_cuda_ext.matmul_int8_to_fp16_scaled_forward_noc(A, B, 1.0)\n",
    "torch.cuda.synchronize()\n",
    "end = time.perf_counter()\n",
    "custom_time = (end - start) / n_iter * 1e3  # in ms\n",
    "print(f\"[custom_matmul_int8_to_fp16] elapsed: {custom_time:.3f} ms \\n\")\n",
    "\n",
    "print(out.dtype)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd6e3beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_bmm_int8] elapsed: 0.290 ms \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare time of int8 matmul\n",
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = torch_cuda_ext.matmul_int8_cutlass(A, B)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "n_iter = 100\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "start.record()\n",
    "for _ in range(n_iter):\n",
    "    out = torch_cuda_ext.matmul_int8_cutlass(A, B)\n",
    "    out = out.to(torch.float16)\n",
    "    \n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "custom_time = start.elapsed_time(end) / n_iter  # in ms\n",
    "print(f\"[custom_bmm_int8] elapsed: {custom_time:.3f} ms \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29dcb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch matmul] elapsed: 0.186 ms\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(-127, 127, (1_000, 2_000), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (2_000, 3_000), dtype=torch.int8, device=\"cuda\")\n",
    "\n",
    "A = A.to(torch.float16)\n",
    "B = B.to(torch.float16)\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(5):\n",
    "    _ = torch.matmul(A, B)\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# Measure time by python time\n",
    "n_iter = 100 \n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iter):\n",
    "    _ = torch.matmul(A, B)\n",
    "torch.cuda.synchronize()\n",
    "end = time.perf_counter()\n",
    "\n",
    "torch_time = (end - start) / n_iter * 1e3  # in ms\n",
    "print(f\"[torch matmul] elapsed: {torch_time:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece48a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
