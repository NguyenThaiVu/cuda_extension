{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae03d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch_cuda_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f27ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom dot product: tensor(8.2927, device='cuda:0')\n",
      "PyTorch dot product: tensor(8.2927, device='cuda:0')\n",
      "Difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(100, device=\"cuda\")\n",
    "b = torch.randn(100, device=\"cuda\")\n",
    "\n",
    "custom_dot = torch_cuda_ext.dot_forward(a, b)\n",
    "torch_dot = torch.dot(a, b)\n",
    "\n",
    "print(\"Custom dot product:\", custom_dot)\n",
    "print(\"PyTorch dot product:\", torch_dot)\n",
    "print(\"Difference:\", torch.abs(custom_dot - torch_dot).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26adb1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correct matmul implementation! ---\n"
     ]
    }
   ],
   "source": [
    "A = torch.randn(10, 20, device=\"cuda\")\n",
    "B = torch.randn(20, 30, device=\"cuda\")\n",
    "\n",
    "custom_matmul = torch_cuda_ext.matmul_f32(A, B)\n",
    "torch_matmul = torch.matmul(A, B)\n",
    "\n",
    "if torch.allclose(custom_matmul, torch_matmul, atol=1e-6):\n",
    "    print(\"--- Correct matmul implementation! ---\")\n",
    "else:\n",
    "    print(\"[ERROR] Incorrect matmul implementation !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f1a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of int8 matmul result: torch.Size([10, 30])\n",
      "Dtype of int8 matmul result: torch.int32\n",
      "--- Correct int8 matmul implementation! ---\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(-127, 127, (10, 20), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (20, 30), dtype=torch.int8, device=\"cuda\")\n",
    "custom_matmul_int8 = torch_cuda_ext.matmul_int8(A, B)\n",
    "\n",
    "print(f\"Shape of int8 matmul result: {custom_matmul_int8.shape}\")\n",
    "print(f\"Dtype of int8 matmul result: {custom_matmul_int8.dtype}\")\n",
    "\n",
    "if torch.allclose(custom_matmul_int8.float(), torch.matmul(A.float(), B.float()), atol=1e-2):\n",
    "    print(\"--- Correct int8 matmul implementation! ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c42bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c461f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of int8 batched matmul result: torch.Size([5, 10, 30])\n",
      "Dtype of int8 batched matmul result: torch.int32\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(-127, 127, (5, 10, 20), dtype=torch.int8, device=\"cuda\")\n",
    "B = torch.randint(-127, 127, (5, 20, 30), dtype=torch.int8, device=\"cuda\")\n",
    "custom_bmatmul_int8 = torch_cuda_ext.bmm_int8(A, B)\n",
    "\n",
    "print(f\"Shape of int8 batched matmul result: {custom_bmatmul_int8.shape}\")\n",
    "print(f\"Dtype of int8 batched matmul result: {custom_bmatmul_int8.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaed86e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 25187.,  -7588.,  -2830.,  ..., -44194.,   5854.,  63146.],\n",
       "         [  5141.,  -7088., -23081.,  ...,   8831.,    118., -29748.],\n",
       "         [ 17396.,    999., -31703.,  ...,  29564.,  -5070., -12355.],\n",
       "         ...,\n",
       "         [  6864.,  -8054.,  22061.,  ...,  32973., -35613., -22143.],\n",
       "         [-10806.,  10242.,  13992.,  ...,  -6467.,  25065.,  12208.],\n",
       "         [-15481.,  -3666., -15239.,  ...,    247.,  -2213.,  -4801.]],\n",
       "\n",
       "        [[-44666.,  31742.,  13281.,  ...,  -5506., -15045.,  -5820.],\n",
       "         [-22354.,  14386., -25246.,  ..., -83878., -24350., -21194.],\n",
       "         [ 14668.,  -5803.,  33727.,  ...,  30368., -28923.,  -5780.],\n",
       "         ...,\n",
       "         [ 48894.,   8602.,   6306.,  ...,  20136.,  -5907., -13481.],\n",
       "         [  4869.,  36882.,   2609.,  ...,  -9629.,  12767., -19111.],\n",
       "         [-27534., -19762.,   9586.,  ...,   -234., -15371.,   -904.]],\n",
       "\n",
       "        [[ 35440.,  -8161.,  -9512.,  ...,  34583.,   5621., -17477.],\n",
       "         [ 25596.,  24572.,  13527.,  ..., -20412., -10188.,  43260.],\n",
       "         [ 25781.,  13007., -13355.,  ..., -14471., -24207.,  19781.],\n",
       "         ...,\n",
       "         [  1320.,   6575.,   4426.,  ...,   2249.,   3613.,   4555.],\n",
       "         [ 49428.,   9469.,   1890.,  ...,   2959.,  41448.,  39068.],\n",
       "         [ 20981.,    568.,   1820.,  ..., -18418.,   9559.,  24057.]],\n",
       "\n",
       "        [[-18495.,  26335., -20373.,  ...,  -9109., -28879., -50385.],\n",
       "         [  1675.,   1502., -51113.,  ...,  32682., -15999.,  -9719.],\n",
       "         [-12845., -36615.,  22155.,  ..., -14917.,  -2648.,   2515.],\n",
       "         ...,\n",
       "         [-46232.,   1129., -20662.,  ...,   -375.,  53614.,  28587.],\n",
       "         [ -3835., -64868.,  -7259.,  ..., -34776., -32173., -27269.],\n",
       "         [ -7633., -35989.,  28564.,  ..., -14618.,   8866.,  -4658.]],\n",
       "\n",
       "        [[ 12994.,   8132.,  27624.,  ..., -37208., -33601.,   9563.],\n",
       "         [  9680.,  -9458.,  19775.,  ..., -37412.,  58041.,  45418.],\n",
       "         [  2226.,   9071.,  30888.,  ...,   1735., -27154.,  -2385.],\n",
       "         ...,\n",
       "         [  9202.,   2269., -11052.,  ..., -10712.,  22068.,  23546.],\n",
       "         [  2497.,  13724.,  16844.,  ...,  18309.,  14211.,    303.],\n",
       "         [ 25215.,  -6692.,  -8860.,  ..., -13054.,  -1485.,  25708.]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bmatmul_int8.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57946308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Correct int8 batched matmul implementation! ---\n"
     ]
    }
   ],
   "source": [
    "if torch.allclose(custom_bmatmul_int8.float(), torch.bmm(A.float(), B.float()), atol=1e-2):\n",
    "    print(\"--- Correct int8 batched matmul implementation! ---\")\n",
    "else:\n",
    "    print(\"[ERROR] Incorrect int8 batched matmul implementation !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ac818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3f8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
