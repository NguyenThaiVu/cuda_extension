{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95da2183",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "In this notebook, I will implement the scaled-dot product operation and compare the speed between \n",
    "- Torch built-in function\n",
    "- Normal bfloat16 scaled-dot product.\n",
    "- Int8 scaled-dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f1c6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import math\n",
    "import time\n",
    "import functools\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "import torch_cuda_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda_memory_profiler(device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Decorator that measures GPU memory usage (and runtime) for any function.\n",
    "    \n",
    "    Reports:\n",
    "      - Δpeak (max temporary memory used)\n",
    "      - Δcurrent (net memory retained after execution)\n",
    "      - runtime (optional)\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            # synchronize before measuring\n",
    "            torch.cuda.synchronize(device)\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats(device)\n",
    "            \n",
    "            before = torch.cuda.memory_allocated(device)\n",
    "\n",
    "            result = func(*args, **kwargs)  # run the function\n",
    "\n",
    "            torch.cuda.synchronize(device)\n",
    "            peak = torch.cuda.max_memory_allocated(device)\n",
    "            \n",
    "            delta_peak = peak - before\n",
    "\n",
    "            msg = (f\"[{func.__name__}] Δpeak: {delta_peak/1e6:.2f} MB\")\n",
    "            print(msg)\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def cuda_time_profiler(print_time=True):\n",
    "    \"\"\"\n",
    "    Decorator to measure GPU execution time of a function using CUDA events.\n",
    "    Works only if at least one tensor is on the CUDA device.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @functools.wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            \n",
    "            # Warm-up\n",
    "            func(*args, **kwargs)\n",
    "            torch.cuda.synchronize() # Ensure all previous CUDA ops are done\n",
    "\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            n_iter = 10\n",
    "\n",
    "            start.record()\n",
    "            for i in range(n_iter):\n",
    "                result = func(*args, **kwargs)\n",
    "            end.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            elapsed_ms = start.elapsed_time(end) / n_iter  # Average time per iteration\n",
    "            if print_time:\n",
    "                print(f\"[{func.__name__}] elapsed: {elapsed_ms:.3f} ms\")\n",
    "\n",
    "            return result\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "def l2_norm(tensor1, tensor2):\n",
    "    return torch.sqrt(torch.nansum((tensor1 - tensor2) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279598c7",
   "metadata": {},
   "source": [
    "# 1. Torch built-in scaled do product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07f7b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "SEQUENCE_LENGTH = 1024 \n",
    "D_MODEL = 512\n",
    "# N_HEADS = 8\n",
    "\n",
    "Q = torch.randn(BATCH_SIZE, SEQUENCE_LENGTH, D_MODEL, device=device, dtype=dtype)\n",
    "K = torch.randn(BATCH_SIZE, SEQUENCE_LENGTH, D_MODEL, device=device, dtype=dtype)\n",
    "V = torch.randn(BATCH_SIZE, SEQUENCE_LENGTH, D_MODEL, device=device, dtype=dtype)\n",
    "\n",
    "# warmup\n",
    "for _ in range(10):\n",
    "    _ = scaled_dot_product_attention(Q, K, V, attn_mask=None, dropout_p=0.0, is_causal=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afb9cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda_memory_profiler()\n",
    "@cuda_time_profiler()\n",
    "def torch_built_in_scaled_dot_product(Q, K, V):\n",
    "    z = scaled_dot_product_attention(Q, K, V)\n",
    "    return z\n",
    "\n",
    "z = torch_built_in_scaled_dot_product(Q, K, V)\n",
    "print(f\"Shape of output: {z.shape}\")\n",
    "print(f\"Dtype of output: {z.dtype}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bed6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48f8d400",
   "metadata": {},
   "source": [
    "# 2. Normal (float) scaled-dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b046874",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda_memory_profiler()\n",
    "@cuda_time_profiler()\n",
    "def custom_scaled_dot_product(Q, K, V):\n",
    "    d_k = Q.size(-1)\n",
    "    scale = 1 / math.sqrt(d_k)\n",
    "    \n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "    attn_weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ccbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_custom = custom_scaled_dot_product(Q, K, V)\n",
    "print(f\"Shape of custom output: {z_custom.shape}\")\n",
    "print(f\"Dtype of custom output: {z_custom.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b52797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "if l2_norm(z, z_custom) < 1.0:\n",
    "    print(\"Outputs are close! - l2 norm difference:\", l2_norm(z, z_custom).item())\n",
    "else:\n",
    "    raise Exception(\"[ERROR] Outputs differ !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b086eec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b76a532",
   "metadata": {},
   "source": [
    "# 3. Int8 scaled-dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8188de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_tensor_asymmetric_int8(mat:torch.Tensor):\n",
    "    \"\"\"\n",
    "    mat: input float tensor (e.g., torch.float32 or torch.bfloat16)\n",
    "    \"\"\"\n",
    "    min_val = mat.min()\n",
    "    max_val = mat.max()\n",
    "    \n",
    "    qmin = -128\n",
    "    qmax = 127\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - torch.round(min_val / scale).item()\n",
    "    \n",
    "    q_mat = torch.clamp(torch.round(mat / scale) + zero_point, qmin, qmax).to(torch.int8)\n",
    "    return q_mat, scale, zero_point\n",
    "\n",
    "def quantize_tensor_symmetric_int8(mat:torch.Tensor):\n",
    "    \"\"\"\n",
    "    Symmetric quantization to int8.\n",
    "    mat: input float tensor (e.g., torch.float32 or torch.bfloat16)\n",
    "    \"\"\"\n",
    "    max_val = torch.max(torch.abs(mat))\n",
    "    \n",
    "    qmin = -128\n",
    "    qmax = 127\n",
    "    scale = max_val / qmax\n",
    "    zero_point = 0  # For symmetric quantization, zero_point is typically 0\n",
    "    \n",
    "    q_mat = torch.clamp(torch.round(mat / scale), qmin, qmax).to(torch.int8)\n",
    "    return q_mat, scale, zero_point\n",
    "\n",
    "def dequantize_tensor_int8(q_mat, scale:float, zero_point:int):\n",
    "    \"\"\"\n",
    "    De-quantize an int8 tensor back to float using the provided scale and zero_point.\n",
    "    q_mat: input int8 tensor\n",
    "    scale: float scaling factor\n",
    "    zero_point: integer zero point\n",
    "    \"\"\"\n",
    "    return scale * (q_mat.float() - zero_point)\n",
    "\n",
    "def quantization_error(original, dequantized):\n",
    "    \"\"\"\n",
    "    Compute the relative error between the original and dequantized tensors using l2 norm.\n",
    "    \"\"\"\n",
    "    return torch.norm(original - dequantized) / torch.norm(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @cuda_memory_profiler()\n",
    "# @cuda_time_profiler()\n",
    "# def scaled_dot_product_int8(Q, K, V):\n",
    "#     dk = Q.size(-1)\n",
    "#     scale = 1.0 / math.sqrt(dk)\n",
    "    \n",
    "#     # Quantize Q, K, V\n",
    "#     Q_q, Q_scale, Q_zp = quantize_tensor_symmetric_int8(Q)\n",
    "#     K_q, K_scale, K_zp = quantize_tensor_symmetric_int8(K)\n",
    "    \n",
    "#     Q_q = Q_q.view(-1, D_MODEL)\n",
    "#     K_q = K_q.view(-1, D_MODEL)\n",
    "\n",
    "#     scores_int32 = int8_linear_matmul(Q_q, K_q, dtype=torch.int32)\n",
    "#     scores = scores_int32.view(BATCH_SIZE, SEQUENCE_LENGTH, SEQUENCE_LENGTH)\n",
    "#     scores = scores.to(dtype) * (Q_scale * K_scale) * scale # dequantize\n",
    "    \n",
    "#     attn_weights = torch.softmax(scores, dim=-1)\n",
    "#     output = torch.matmul(attn_weights, V)\n",
    "#     return output\n",
    "\n",
    "\n",
    "@cuda_memory_profiler()\n",
    "@cuda_time_profiler()\n",
    "def scaled_dot_product_int8(Q_q, Q_scale, K_q, K_scale, V):\n",
    "    dk = Q_q.size(-1)\n",
    "    scale = 1.0 / math.sqrt(dk)\n",
    "    \n",
    "    scores_int32 = torch_cuda_ext.bmm_int8(Q_q, K_q)\n",
    "    scores = scores_int32.to(dtype) * (Q_scale * K_scale) * scale # dequantize\n",
    "    \n",
    "    attn_weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantize Q, K, V\n",
    "Q_q, Q_scale, Q_zp = quantize_tensor_symmetric_int8(Q)\n",
    "K_q, K_scale, K_zp = quantize_tensor_symmetric_int8(K)\n",
    "K_q_transpose = K_q.transpose(-2, -1).contiguous()\n",
    "\n",
    "z_int8 = scaled_dot_product_int8(Q_q, Q_scale, K_q_transpose, K_scale, V)\n",
    "print(f\"Shape of int8 output: {z_int8.shape}\")\n",
    "print(f\"Dtype of int8 output: {z_int8.dtype}\")\n",
    "\n",
    "print()\n",
    "if l2_norm(z, z_int8) / BATCH_SIZE < 1.0:\n",
    "    print(\"Int8 Outputs are close! - l2 norm difference:\", l2_norm(z, z_int8).item())\n",
    "else:\n",
    "    print(\"L2 norm difference:\", l2_norm(z, z_int8).item())\n",
    "    raise Exception(\"[ERROR] Int8 Outputs differ !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f84d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3be209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
